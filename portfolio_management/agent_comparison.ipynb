{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from agents import agent_loss, sampled_agent_reward, CAPM_Agent, MVP_Agent, Uniform_Agent\r\n",
    "from trading_env.environment import TradingEnv\r\n",
    "\r\n",
    "import dill\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from scipy import stats\r\n",
    "import seaborn as sns \r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recall data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('test_data.dill','rb') as dill_file:\r\n",
    "    test_dataset = dill.load(dill_file)\r\n",
    "test_dates = test_dataset['dates']\r\n",
    "test_data = test_dataset['data']\r\n",
    "\r\n",
    "n_stocks = test_data.shape[0]\r\n",
    "\r\n",
    "env = TradingEnv(test_data)\r\n",
    "start = env._start_tick"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recall trained agent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('dpm_agent.dill','rb') as dill_file:\r\n",
    "    dpm_agent = dill.load(dill_file)\r\n",
    "\r\n",
    "capm_agent = CAPM_Agent()\r\n",
    "mvp_agent = MVP_Agent()\r\n",
    "uniform_agent = Uniform_Agent()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run a single simulation on all stock data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env = TradingEnv(test_data)\r\n",
    "\r\n",
    "dpm_loss = agent_loss(env,dpm_agent)\r\n",
    "dpm_val_hist = env.portfolio_value_hist\r\n",
    "\r\n",
    "capm_loss = agent_loss(env,capm_agent)\r\n",
    "capm_val_hist = env.portfolio_value_hist\r\n",
    "\r\n",
    "mvp_loss = agent_loss(env,mvp_agent)\r\n",
    "mvp_val_hist = env.portfolio_value_hist\r\n",
    "\r\n",
    "uniform_loss = agent_loss(env,uniform_agent)\r\n",
    "uniform_val_hist = env.portfolio_value_hist\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(test_dates[start:],dpm_val_hist,label='Deep Portfolio Management')\r\n",
    "plt.plot(test_dates[start:],capm_val_hist,label='Capital Asset Pricing Model')\r\n",
    "plt.plot(test_dates[start:],mvp_val_hist,label='Minimum Variance Portfolio')\r\n",
    "plt.plot(test_dates[start:],uniform_val_hist,label='Uniform Portfolio Weights')\r\n",
    "plt.xticks(rotation=45)\r\n",
    "plt.legend(loc='upper right',bbox_to_anchor=(1.6, 1) )\r\n",
    "plt.title('Comparison of several investment strategies.')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Statistical analysis\r\n",
    "\r\n",
    "One simulation is interesting to view but it is important to compare the statistical performance of the agents. Here we will simulate several trials and perform a simple t-test on the results to compare agents. Note the randomization is in the subset of stocks chosen in each trial. On each trial we select with replacement a configuration of 10 out of 36 stocks on which to run simulations. There is a total of ${36 \\choose 10} \\approx 9.1 \\times 10^{9}$ possible configurations. On each trial the configuration is fed into each agent, thus yielding a paired t-test experiment design."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "TRIALS = 100\r\n",
    "n_group_size = 8\r\n",
    "\r\n",
    "capm_agent = CAPM_Agent()\r\n",
    "mvp_agent = MVP_Agent()\r\n",
    "uniform_agent = Uniform_Agent()\r\n",
    "agents = [dpm_agent,\r\n",
    "            capm_agent,\r\n",
    "            mvp_agent,\r\n",
    "            uniform_agent]\r\n",
    "\r\n",
    "rewards = sampled_agent_reward(TradingEnv,test_data,agents,n_group_size,TRIALS)     \r\n",
    "\r\n",
    "rewards_df = pd.DataFrame(rewards).astype(float)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f'The rewards_df dataframe contains {rewards_df.shape[0]} rows ',end='') \r\n",
    "print('representing normalized annual returns for each agent for each ', end = '')\r\n",
    "print('selected configuration.' )\r\n",
    "rewards_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize the results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#final_vals_df = pd.DataFrame(final_vals).astype(float)\r\n",
    "\r\n",
    "sns.boxplot(data=rewards_df)\r\n",
    "plt.title('Average annual returns.')\r\n",
    "plt.show()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Mean annual returns for each agent.')\r\n",
    "rewards_df.mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rewards_df.hist()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## t-test\r\n",
    "\r\n",
    "It seems from the above visualizations that the DeepPortfolio agent outperforms the Uniform agent, but can we be sure about that?\r\n",
    "\r\n",
    "We perform a t-tests with the alternative hypotheses that DeepPortfolio outperforms the Uniform portfolio."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "stats.ttest_rel(rewards_df['DPM_Agent'],rewards_df['Uniform_Agent'] ,alternative = 'greater')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The p value is very small, so we can easily reject the null hypothesis and conclude DeepPortfolio outperforms the Uniform agent.\r\n",
    " \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6ecf0f4b7da91ff1ccddb66793d3b78ca4cd194ae4db75acbe588f9b304d562"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('portfolio': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}