{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.dpm_agent import DPMAgent\n",
    "from trading_env.environment import TradingEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yf_file = \"./data/archive_data/yf_data.dill\"\n",
    "with open(yf_file,'rb') as dill_file:\n",
    "    yf_df = dill.load(dill_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = yf_df['Symbol'].unique()\n",
    "stocks = []\n",
    "for symbol in symbols:\n",
    "    df = yf_df[yf_df['Symbol'] == symbol]\n",
    "    stocks.append(df.iloc[:,2:6].to_numpy())\n",
    "stocks = np.array(stocks)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "def get_advantages(values, masks, rewards):\n",
    "    gamma = 0.99\n",
    "    lmbda = 0.95\n",
    "    critic_discount = 0.5\n",
    "\n",
    "    returns = []\n",
    "    gae = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + gamma * values[i + 1] * masks[i] - values[i]\n",
    "        gae = delta + gamma * lmbda * masks[i] * gae\n",
    "        returns.insert(0, gae + values[i])\n",
    "\n",
    "    adv = np.array(returns) - values[:-1]\n",
    "    return returns, (adv - np.mean(adv)) / (np.std(adv) + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = DPMAgent(21,4,1,'actor')\n",
    "critic = DPMAgent(21,4,1,'critic')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reward():\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    print('testing...')\n",
    "    limit = 0\n",
    "    while not done:\n",
    "        state_input = K.expand_dims(state, 0)\n",
    "        action_probs = model_actor.predict([state_input, dummy_n, dummy_1, dummy_1, dummy_1], steps=1)\n",
    "        action = np.argmax(action_probs)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        limit += 1\n",
    "        if limit > 20:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(probs):\n",
    "    one_hot = np.zeros_like(probs)\n",
    "    one_hot[:, np.argmax(probs, axis=1)] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnv(stocks)\n",
    "obs = env.reset()\n",
    "state_dims = env.observation_space.shape\n",
    "n_actions = env.action_space.shape[0]\n",
    "\n",
    "dummy_n = np.zeros((1, 1, n_actions))\n",
    "dummy_1 = np.zeros((1, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_state = obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(22,)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_board = TensorBoard(log_dir='./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_steps = 128\n",
    "target_reached = False\n",
    "best_reward = 0\n",
    "iters = 0\n",
    "max_iters = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "itr: 0, action=16, reward=0.0025982670979001147, q val=[[-0.2037347]]\n",
      "itr: 1, action=6, reward=-0.007858264446218835, q val=[[-0.2037347]]\n",
      "itr: 2, action=19, reward=0.004422573840911233, q val=[[-0.2037347]]\n",
      "itr: 3, action=7, reward=0.0047424370505121365, q val=[[-0.2037347]]\n",
      "itr: 4, action=17, reward=0.00691183404786655, q val=[[-0.2037347]]\n",
      "itr: 5, action=0, reward=-0.005909008021246886, q val=[[-0.2037347]]\n",
      "itr: 6, action=0, reward=0.0020144899926779823, q val=[[-0.2037347]]\n",
      "itr: 7, action=2, reward=-0.02764351333310642, q val=[[-0.2037347]]\n",
      "itr: 8, action=0, reward=-0.03819000663464601, q val=[[-0.2037347]]\n",
      "itr: 9, action=17, reward=-0.017453876375786305, q val=[[-0.2037347]]\n",
      "itr: 10, action=15, reward=0.030895193373914075, q val=[[-0.2037347]]\n",
      "itr: 11, action=9, reward=0.004481912475622163, q val=[[-0.2037347]]\n",
      "itr: 12, action=10, reward=-0.006514291667175617, q val=[[-0.2037347]]\n",
      "itr: 13, action=16, reward=0.004926753619528598, q val=[[-0.2037347]]\n",
      "itr: 14, action=1, reward=0.016702376191357247, q val=[[-0.2037347]]\n",
      "itr: 15, action=9, reward=0.00635484448555793, q val=[[-0.2037347]]\n",
      "itr: 16, action=13, reward=-0.017594769870402004, q val=[[-0.2037347]]\n",
      "itr: 17, action=21, reward=-0.0020371112502326105, q val=[[-0.2037347]]\n",
      "itr: 18, action=7, reward=0.006542599839071303, q val=[[-0.2037347]]\n",
      "itr: 19, action=9, reward=-0.0007611881942760132, q val=[[-0.2037347]]\n",
      "itr: 20, action=18, reward=-0.026375495817011485, q val=[[-0.2037347]]\n",
      "itr: 21, action=19, reward=-0.00581840348889789, q val=[[-0.2037347]]\n",
      "itr: 22, action=3, reward=0.0007817978569791777, q val=[[-0.2037347]]\n",
      "itr: 23, action=20, reward=0.005943707558642322, q val=[[-0.2037347]]\n",
      "itr: 24, action=1, reward=-0.0018933445840731869, q val=[[-0.2037347]]\n",
      "itr: 25, action=15, reward=-0.012935949917574314, q val=[[-0.2037347]]\n",
      "itr: 26, action=13, reward=0.003167052624129154, q val=[[-0.2037347]]\n",
      "itr: 27, action=15, reward=0.008209441653804923, q val=[[-0.2037347]]\n",
      "itr: 28, action=18, reward=0.007631544864870107, q val=[[-0.2037347]]\n",
      "itr: 29, action=12, reward=0.020623194631979275, q val=[[-0.2037347]]\n",
      "itr: 30, action=0, reward=-0.007574762637549263, q val=[[-0.2037347]]\n",
      "itr: 31, action=9, reward=9.486493808467509e-06, q val=[[-0.2037347]]\n",
      "itr: 32, action=17, reward=-0.02300551639707385, q val=[[-0.2037347]]\n",
      "itr: 33, action=19, reward=-0.006258060689853741, q val=[[-0.2037347]]\n",
      "itr: 34, action=8, reward=0.002599049354287442, q val=[[-0.2037347]]\n",
      "itr: 35, action=12, reward=-0.0005877421374438121, q val=[[-0.2037347]]\n",
      "itr: 36, action=18, reward=0.0018473123852759997, q val=[[-0.2037347]]\n",
      "itr: 37, action=9, reward=-0.017829484656992797, q val=[[-0.2037347]]\n",
      "itr: 38, action=9, reward=0.01289331171704105, q val=[[-0.2037347]]\n",
      "itr: 39, action=18, reward=0.017532914387851928, q val=[[-0.2037347]]\n",
      "itr: 40, action=18, reward=0.002593578902530307, q val=[[-0.2037347]]\n",
      "itr: 41, action=9, reward=0.03574331083336147, q val=[[-0.2037347]]\n",
      "itr: 42, action=6, reward=-0.0035043534298228925, q val=[[-0.2037347]]\n",
      "itr: 43, action=16, reward=-0.004703018702813899, q val=[[-0.2037347]]\n",
      "itr: 44, action=8, reward=-0.006283899932841217, q val=[[-0.2037347]]\n",
      "itr: 45, action=2, reward=-0.007455662231642913, q val=[[-0.2037347]]\n",
      "itr: 46, action=12, reward=0.000761066127995728, q val=[[-0.2037347]]\n",
      "itr: 47, action=4, reward=-0.017822846096408745, q val=[[-0.2037347]]\n",
      "itr: 48, action=13, reward=0.0038034254129491163, q val=[[-0.2037347]]\n",
      "itr: 49, action=0, reward=0.009925003580877006, q val=[[-0.2037347]]\n",
      "itr: 50, action=12, reward=6.326118063925337e-05, q val=[[-0.2037347]]\n",
      "itr: 51, action=2, reward=0.0008489820154289997, q val=[[-0.2037347]]\n",
      "itr: 52, action=5, reward=0.002845377317146549, q val=[[-0.2037347]]\n",
      "itr: 53, action=9, reward=-0.009263733861405178, q val=[[-0.2037347]]\n",
      "itr: 54, action=13, reward=0.0034903164287335003, q val=[[-0.2037347]]\n",
      "itr: 55, action=11, reward=-0.0014769914363487353, q val=[[-0.2037347]]\n",
      "itr: 56, action=2, reward=-0.019595888850265756, q val=[[-0.2037347]]\n",
      "itr: 57, action=0, reward=0.008745317456607305, q val=[[-0.2037347]]\n",
      "itr: 58, action=3, reward=0.013728151905655862, q val=[[-0.2037347]]\n",
      "itr: 59, action=15, reward=-0.0007317952998114819, q val=[[-0.2037347]]\n",
      "itr: 60, action=15, reward=-0.012058406052266732, q val=[[-0.2037347]]\n",
      "itr: 61, action=12, reward=0.002582863960191892, q val=[[-0.2037347]]\n",
      "itr: 62, action=5, reward=-0.002727695670494887, q val=[[-0.2037347]]\n",
      "itr: 63, action=10, reward=0.0036287295743679363, q val=[[-0.2037347]]\n",
      "itr: 64, action=9, reward=-0.0035664108792030375, q val=[[-0.2037347]]\n",
      "itr: 65, action=3, reward=0.018695502019745944, q val=[[-0.2037347]]\n",
      "itr: 66, action=15, reward=0.011265670256345933, q val=[[-0.2037347]]\n",
      "itr: 67, action=14, reward=0.004676676227175199, q val=[[-0.2037347]]\n",
      "itr: 68, action=12, reward=0.00671369117450511, q val=[[-0.2037347]]\n",
      "itr: 69, action=16, reward=0.0024276548521816516, q val=[[-0.2037347]]\n",
      "itr: 70, action=2, reward=0.004374431107240911, q val=[[-0.2037347]]\n",
      "itr: 71, action=10, reward=0.005764106854978733, q val=[[-0.2037347]]\n",
      "itr: 72, action=7, reward=-0.003601880807870889, q val=[[-0.2037347]]\n",
      "itr: 73, action=13, reward=-0.006885873395292087, q val=[[-0.2037347]]\n",
      "itr: 74, action=2, reward=0.007391957324988591, q val=[[-0.2037347]]\n",
      "itr: 75, action=9, reward=-0.008192536125582527, q val=[[-0.2037347]]\n",
      "itr: 76, action=17, reward=-0.007033730864799914, q val=[[-0.2037347]]\n",
      "itr: 77, action=16, reward=-0.010586570836040693, q val=[[-0.2037347]]\n",
      "itr: 78, action=19, reward=-0.008102718693076338, q val=[[-0.2037347]]\n",
      "itr: 79, action=2, reward=-0.008304424361056082, q val=[[-0.2037347]]\n",
      "itr: 80, action=18, reward=-0.0006148966364866731, q val=[[-0.2037347]]\n",
      "itr: 81, action=12, reward=-0.010896133440483305, q val=[[-0.2037347]]\n",
      "itr: 82, action=21, reward=0.0013724096013720372, q val=[[-0.2037347]]\n",
      "itr: 83, action=21, reward=0.00541772883591505, q val=[[-0.2037347]]\n",
      "itr: 84, action=0, reward=0.011958887168730593, q val=[[-0.2037347]]\n",
      "itr: 85, action=12, reward=0.00749705567245746, q val=[[-0.2037347]]\n",
      "itr: 86, action=7, reward=0.01172465767872598, q val=[[-0.2037347]]\n",
      "itr: 87, action=15, reward=0.010272420294120706, q val=[[-0.2037347]]\n",
      "itr: 88, action=5, reward=-0.00014869453047694844, q val=[[-0.2037347]]\n",
      "itr: 89, action=20, reward=-0.003618362996809435, q val=[[-0.2037347]]\n",
      "itr: 90, action=16, reward=0.0026091471075325528, q val=[[-0.2037347]]\n",
      "itr: 91, action=2, reward=0.008876450006673059, q val=[[-0.2037347]]\n",
      "itr: 92, action=6, reward=0.004307651671189328, q val=[[-0.2037347]]\n",
      "itr: 93, action=2, reward=-0.004752194393079456, q val=[[-0.2037347]]\n",
      "itr: 94, action=9, reward=0.0021679574796943835, q val=[[-0.2037347]]\n",
      "itr: 95, action=20, reward=0.0012546958522285493, q val=[[-0.2037347]]\n",
      "itr: 96, action=17, reward=0.00041569414712166254, q val=[[-0.2037347]]\n",
      "itr: 97, action=8, reward=0.001080815027812917, q val=[[-0.2037347]]\n",
      "itr: 98, action=1, reward=-0.0010099250658903813, q val=[[-0.2037347]]\n",
      "itr: 99, action=3, reward=0.00689109600538675, q val=[[-0.2037347]]\n",
      "itr: 100, action=16, reward=0.0015214316822786355, q val=[[-0.2037347]]\n",
      "itr: 101, action=21, reward=0.009122802612294685, q val=[[-0.2037347]]\n",
      "itr: 102, action=2, reward=-0.008374072597602231, q val=[[-0.2037347]]\n",
      "itr: 103, action=10, reward=0.00018841517907939882, q val=[[-0.2037347]]\n",
      "itr: 104, action=15, reward=0.004848918022308995, q val=[[-0.2037347]]\n",
      "itr: 105, action=1, reward=0.0017195248641735678, q val=[[-0.2037347]]\n",
      "itr: 106, action=7, reward=0.005633042678948617, q val=[[-0.2037347]]\n",
      "itr: 107, action=7, reward=0.008330221620516134, q val=[[-0.2037347]]\n",
      "itr: 108, action=13, reward=-0.004509622826537688, q val=[[-0.2037347]]\n",
      "itr: 109, action=0, reward=-0.002000577494276576, q val=[[-0.2037347]]\n",
      "itr: 110, action=15, reward=0.0007514443260242779, q val=[[-0.2037347]]\n",
      "itr: 111, action=16, reward=0.0031016587495024677, q val=[[-0.2037347]]\n",
      "itr: 112, action=7, reward=-0.003944627140748506, q val=[[-0.2037347]]\n",
      "itr: 113, action=9, reward=-0.002443252433654863, q val=[[-0.2037347]]\n",
      "itr: 114, action=3, reward=-0.010038449672281832, q val=[[-0.2037347]]\n",
      "itr: 115, action=13, reward=-0.007445459403125129, q val=[[-0.2037347]]\n",
      "itr: 116, action=7, reward=-0.003945596644956553, q val=[[-0.2037347]]\n",
      "itr: 117, action=6, reward=-0.01119363510255661, q val=[[-0.2037347]]\n",
      "itr: 118, action=21, reward=0.005771865439134262, q val=[[-0.2037347]]\n",
      "itr: 119, action=0, reward=-0.006622922996683063, q val=[[-0.2037347]]\n",
      "itr: 120, action=3, reward=0.008923408371329718, q val=[[-0.2037347]]\n",
      "itr: 121, action=6, reward=-0.0071190267405603245, q val=[[-0.2037347]]\n",
      "itr: 122, action=12, reward=-0.003673616865681609, q val=[[-0.2037347]]\n",
      "itr: 123, action=12, reward=0.007612522583059869, q val=[[-0.2037347]]\n",
      "itr: 124, action=14, reward=-0.02066522637532487, q val=[[-0.2037347]]\n",
      "itr: 125, action=15, reward=-0.007675933143369698, q val=[[-0.2037347]]\n",
      "itr: 126, action=5, reward=-0.003751622867512893, q val=[[-0.2037347]]\n",
      "itr: 127, action=8, reward=-0.010495641622031069, q val=[[-0.2037347]]\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:788 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:754 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1012 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:424 call\n        return self._run_internal_graph(\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:560 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1012 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\merge.py:183 call\n        return self._merge_function(inputs)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\merge.py:522 _merge_function\n        return K.concatenate(inputs, axis=self.axis)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\backend.py:2989 concatenate\n        return array_ops.concat([to_dense(x) for x in tensors], axis)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1677 concat\n        return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:1206 concat_v2\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py:590 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:3528 _create_op_internal\n        ret = Operation(\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:2015 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:1856 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimension 0 in both shapes must be equal, but are 32 and 1. Shapes are [32,21,1] and [1,21,1]. for '{{node model/concatenate/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](model/conv2/Relu, model/Cast, model/concatenate/concat/axis)' with input shapes: [32,21,1,20], [1,21,1,1], [] and with computed input tensors: input[2] = <3>.\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-8edef572bc0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mactions_onehot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions_onehot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     actor_loss = actor.model.fit(\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madvantages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mactions_onehot\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 725\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    726\u001b[0m             *args, **kwds))\n\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3196\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:788 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:754 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1012 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:424 call\n        return self._run_internal_graph(\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:560 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1012 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\merge.py:183 call\n        return self._merge_function(inputs)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\merge.py:522 _merge_function\n        return K.concatenate(inputs, axis=self.axis)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\backend.py:2989 concatenate\n        return array_ops.concat([to_dense(x) for x in tensors], axis)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1677 concat\n        return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:1206 concat_v2\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py:590 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:3528 _create_op_internal\n        ret = Operation(\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:2015 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    C:\\Users\\rajin\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:1856 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimension 0 in both shapes must be equal, but are 32 and 1. Shapes are [32,21,1] and [1,21,1]. for '{{node model/concatenate/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](model/conv2/Relu, model/Cast, model/concatenate/concat/axis)' with input shapes: [32,21,1,20], [1,21,1,1], [] and with computed input tensors: input[2] = <3>.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "while not target_reached and iters < max_iters:\n",
    "\n",
    "    states = []\n",
    "    actions = []\n",
    "    values = []\n",
    "    masks = []\n",
    "    rewards = []\n",
    "    actions_probs = []\n",
    "    actions_onehot = []\n",
    "    state_input = None\n",
    "    for itr in range(ppo_steps):\n",
    "        state_input = K.expand_dims(stock_state, 0)\n",
    "        action_dist = actor.model.predict([state_input, dummy_n, dummy_1, dummy_1, dummy_1], steps=1)\n",
    "        q_value = critic.model.predict([state_input], steps=1)\n",
    "        action = np.random.choice(n_actions, p=action_dist[0, :])\n",
    "        action_onehot = np.zeros(n_actions)\n",
    "        action_onehot[action] = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        observation, reward, done, info = env.step(action_dist)\n",
    "        print('itr: ' + str(itr) + ', action=' + str(action) + ', reward=' + str(reward) + ', q val=' + str(q_value))\n",
    "        mask = not done\n",
    "\n",
    "        states.append(stock_state)\n",
    "        actions.append(action)\n",
    "        actions_onehot.append(action_onehot)\n",
    "        values.append(q_value)\n",
    "        masks.append(mask)\n",
    "        rewards.append(reward)\n",
    "        actions_probs.append(action_dist)\n",
    "\n",
    "        stock_state = observation \n",
    "\n",
    "        if done:\n",
    "            env.reset()\n",
    "    \n",
    "\n",
    "    q_value = critic.model.predict(state_input, steps=1)\n",
    "    values.append(q_value)\n",
    "    returns, advantages = get_advantages(values, masks, rewards)\n",
    "\n",
    "    states = np.array(states)\n",
    "    actions_probs = np.array(actions_probs)\n",
    "    rewards = np.reshape(rewards, newshape=(-1, 1, 1))\n",
    "    values = np.array(values[:-1])\n",
    "    actions_onehot = np.reshape(actions_onehot, newshape=(-1, n_actions))\n",
    "\n",
    "    actor_loss = actor.model.fit(\n",
    "        [states, actions_probs, advantages, rewards, values],\n",
    "        [actions_onehot], verbose=True, shuffle=True, epochs=8,\n",
    "        callbacks=[tensor_board])\n",
    "    critic_loss = model_critic.fit([states], [np.reshape(returns, newshape=(-1, 1))], shuffle=True, epochs=8,\n",
    "                                   verbose=True, callbacks=[tensor_board])\n",
    "\n",
    "    avg_reward = np.mean([test_reward() for _ in range(5)])\n",
    "    print('total test reward=' + str(avg_reward))\n",
    "    if avg_reward > best_reward:\n",
    "        print('best reward=' + str(avg_reward))\n",
    "        actor.model.save('model_actor_{}_{}.hdf5'.format(iters, avg_reward))\n",
    "        critic.model.save('model_critic_{}_{}.hdf5'.format(iters, avg_reward))\n",
    "        best_reward = avg_reward\n",
    "    if best_reward > 0.9 or iters > max_iters:\n",
    "        target_reached = True\n",
    "    iters += 1\n",
    "    env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(128, 21, 64, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(128, 1, 22)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "actions_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(128, 1, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "advantages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd03fd04f7af2b494cfcb6f7f88dc7e2e0996fcff442ebb77a6d3107b9d139ab71e",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}